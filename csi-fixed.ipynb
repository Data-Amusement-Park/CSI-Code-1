{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI: A hybrid deep model for fake news detection\n",
    "\n",
    "## Temporal event modeling based on LSTM\n",
    "\n",
    "    - Event(article) : a series of tweets(messages) which can be Fake/Real\n",
    "    - Message : Temporal behavior on a certain event. (aka tweet)\n",
    "        - Each message arrives at a certain timestamp.\n",
    "        - Each message has auxiliary information such as text, user_info , etc.\n",
    "    - Dataset\n",
    "        - Weibo : 4,664 events, (2,313/2,351 Rumor/NonRumor), 3,805,656 posts, 2,746,818 users\n",
    "                Avg. # of posts/event : 816\n",
    "                Max # of posts/event : 59,318\n",
    "                Min # of posts/event : 10\n",
    "\n",
    "### Description\n",
    "    - input: dictionary with structure below\n",
    "        event_name: {\n",
    "                timestamps : sorted array of timestamps,\n",
    "                text: array of tweets' texts sorted by time,\n",
    "                uid: user ids involved in event,\n",
    "                label: 0(real)/1(fake)\n",
    "                }\n",
    "    - output: label\n",
    "   \n",
    "### Attention: This notebook is for weibo dataset. you can modify the dictionary part for other datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import jieba, re\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating input dictionary\n",
    "modify this part to test model on your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bd9e24b0e3473396590208b1a79d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4232), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#loading events labels\n",
    "f = open('../Data/Weibo.txt', \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "events = {}\n",
    "for line in lines:\n",
    "    line = line.replace('\\t',' ')\n",
    "    line = line.split(' ')\n",
    "    line.remove('\\n')\n",
    "\n",
    "    label = line[1][-1]\n",
    "    eid = line[0][4:]\n",
    "    events[eid] = label\n",
    "    \n",
    "\n",
    "path = '../Data/Weibo'\n",
    "train_dict_ = {}\n",
    "test_dict_ = {}\n",
    "\n",
    "\n",
    "# defining train, test , validation datasets\n",
    "splits = json.load(open('../Data/Weiboeids','r'))\n",
    "eid_train = splits['train']\n",
    "eid_test = splits['test']\n",
    "eid_val = splits['validation']\n",
    "\n",
    "# all eids should be string\n",
    "#making the input dictionary based on what is defined above\n",
    "\n",
    "for event in tqdm(eid_train+eid_test):\n",
    "    event_str = str(event)\n",
    "    tweets = json.load(open(os.path.join(path,event_str+'.json'),\"r\",encoding='utf-8'))\n",
    "    timestamps = []\n",
    "    uid = []\n",
    "    text = []\n",
    "    n = len(tweets)\n",
    "    index = 100\n",
    "    if event in eid_test:\n",
    "        index = 10\n",
    "    tweets = tweets[:index]\n",
    "    for tweet in tweets:\n",
    "        timestamps.append(tweet['t'])\n",
    "        uid.append(tweet['uid'])\n",
    "        text.append(tweet['text'])\n",
    "    messages = {'timestamps':timestamps,'uid':uid,'text':text,'label':events[event_str]}\n",
    "    if event in eid_train:\n",
    "        train_dict_[event_str] = messages\n",
    "    else:\n",
    "        test_dict_[event_str] = messages\n",
    "        \n",
    "# saving the dictionary\n",
    "# ft = open('dict_.json','w')\n",
    "# json.dump(dict_,ft)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train events : 3777\n",
      "#train users : 306494\n",
      "#train messages : 347529\n",
      "\n",
      "#test events : 455\n",
      "#test users : 4335\n",
      "#test messages : 4550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def get_stats(dict_):\n",
    "    nb_messages = []\n",
    "    user_set = set()\n",
    "    list_lengths = []\n",
    "    for eid, messages in dict_.items():\n",
    "        nb_messages.append(len(messages['timestamps']))\n",
    "        user_set.update(messages['uid'])\n",
    "        ts = np.array(messages['timestamps'], dtype=np.float32)\n",
    "        list_lengths.append(ts[-1]-ts[0])\n",
    "        \n",
    "    return nb_messages, user_set, list_lengths\n",
    "\n",
    "    \n",
    "# Get statistics\n",
    "# twitter eid is 'E741' or 'TM1211'\n",
    "# weibo eid is , 3906982031327232 (use as string)\n",
    "nb_messages_train, user_set_train, list_lengths_train = get_stats(train_dict_) \n",
    "nb_messages_test, user_set_test, list_lengths_test = get_stats(test_dict_) \n",
    "\n",
    "print(\"#train events : {}\".format(len(eid_train)))\n",
    "print(\"#train users : {}\".format(len(user_set_train)))\n",
    "print(\"#train messages : {}\".format(np.sum(nb_messages_train)))\n",
    "print()\n",
    "print(\"#test events : {}\".format(len(eid_test)))\n",
    "print(\"#test users : {}\".format(len(user_set_test)))\n",
    "print(\"#test messages : {}\".format(np.sum(nb_messages_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get two sets of user features\n",
    "- `u_sample` : list of tuples (user_ID, # appearances). Top-K users are sampled. \n",
    "- `u_pop` : list of tuples (user_ID, # appearances). All users are sampled. \n",
    "- `user_feature` ($U$$\\Sigma$) : comes from user x event matrix, $M = U$ $\\Sigma$$V^T$.\n",
    "    - $M[i,j]$ : 1 if i-th user from `u_pop` interacts with j-th event. Otherwise, 0.\n",
    "- `user_feature_sub` ($U'$$\\Sigma'$) : comes from user x user matrix, $M' = U'$ $\\Sigma'$$V'^T$.\n",
    "    - $M' = PP^T$\n",
    "    - $P[i,j]$ : 1 if i-th user from `u_sample` interacts with j-th event. Otherwise, 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test u_sample for most common 20000 users is obtained.\n",
      "test u_pop for all 4335 users is obtained.\n",
      "\n",
      "test:\n",
      "# users in u_sample : 4335\n",
      "# users : 4335\n",
      "# events : 455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af34bb7a1a7c419595719d6491d4ae81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 events have at least one user in u_sample\n",
      "0 events have no user in u_sample\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13b1a6a08aa45619f28ef4844e54299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 events have at least one user in u_sample\n",
      "0 events have no user in u_sample\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36717dfb604c4f1c932ea848a29d936a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 events have at least one user in u_sample\n",
      "0 events have no user in u_sample\n",
      "test:\n",
      "matrix_sub shape : (4335, 455)\n",
      "Sparsity : 0.0022743577068836582\n",
      "matrix_main shape : (4335, 455)\n",
      "Sparsity : 0.0022743577068836582\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_Usample(dict_, most_common=50):\n",
    "    '''Get U_sample who are most_common.'''\n",
    "    u_sample = []\n",
    "    cnt = Counter()\n",
    "    for ii, (eid, value) in enumerate(dict_.items()):\n",
    "        users = value['uid']\n",
    "        cnt.update(users)\n",
    "    return cnt.most_common(most_common)    # [(user_id, #occur in all events), ...]\n",
    "\n",
    "def get_user_in_event(dict_, eid, u_sample):\n",
    "    '''Get users who acts on a given event, eid'''\n",
    "    value = dict_[eid]\n",
    "    cnt = Counter(value['uid'])\n",
    "    users = set(value['uid'])\n",
    "    user_in_event = []\n",
    "    for uid, nb_occur in u_sample:\n",
    "        if uid in users:\n",
    "            user_in_event.append((uid, cnt[uid]))\n",
    "    return user_in_event\n",
    "\n",
    "threshold = 20000\n",
    "u_sample_train = get_Usample(train_dict_, most_common=threshold)\n",
    "u_pop_train = get_Usample(train_dict_, most_common=len(user_set_train))\n",
    "print(\"train u_sample for most common {} users is obtained.\".format(threshold))\n",
    "print(\"train u_pop for all {} users is obtained.\".format(len(user_set_train)))\n",
    "\n",
    "u_sample_test = get_Usample(test_dict_, most_common=threshold)\n",
    "u_pop_test = get_Usample(test_dict_, most_common=len(user_set_test))\n",
    "print(\"test u_sample for most common {} users is obtained.\".format(threshold))\n",
    "print(\"test u_pop for all {} users is obtained.\".format(len(user_set_test)))\n",
    "\n",
    "\n",
    "'''\n",
    "Here are Two user-event matrices.\n",
    "    1) matrix_main : (all user - all event) relation\n",
    "        It has #occurrences of a user(row) in an event(col)\n",
    "        It is very sparse.\n",
    "        It is decomposed with smaller K.\n",
    "    2) matrix_sub : (u_sample - eid_sample) relation\n",
    "        It has #occurrences of a user(row) in an event(col)\n",
    "        It is denser.\n",
    "        It is decomposed with larger K. (usually)\n",
    "'''\n",
    "#train\n",
    "print('train:')\n",
    "user_sample2ind_train = {}\n",
    "for ii, (uid, nb_occur) in enumerate(u_sample_train):\n",
    "    user_sample2ind_train[uid] = ii\n",
    "print(\"# users in u_sample : {}\".format(len(user_sample2ind_train)))\n",
    "user2ind_train = {}\n",
    "for ii, uid in enumerate(user_set_train):\n",
    "    user2ind_train[uid] = ii\n",
    "print(\"# users : {}\".format(len(user2ind_train)))\n",
    "eid2ind_train = {}\n",
    "for ii, eid in enumerate(eid_train):\n",
    "    eid2ind_train[eid] = ii\n",
    "print(\"# events : {}\".format(len(eid2ind_train)))\n",
    "\n",
    "#test\n",
    "print('\\ntest:')\n",
    "user_sample2ind_test = {}\n",
    "for ii, (uid, nb_occur) in enumerate(u_sample_test):\n",
    "    user_sample2ind_test[uid] = ii\n",
    "print(\"# users in u_sample : {}\".format(len(user_sample2ind_test)))\n",
    "user2ind_test = {}\n",
    "for ii, uid in enumerate(user_set_test):\n",
    "    user2ind_test[uid] = ii\n",
    "print(\"# users : {}\".format(len(user2ind_test)))\n",
    "eid2ind_test = {}\n",
    "for ii, eid in enumerate(eid_test):\n",
    "    eid2ind_test[eid] = ii\n",
    "print(\"# events : {}\".format(len(eid2ind_test)))\n",
    "\n",
    "\n",
    "'''\n",
    "User Features\n",
    "    Generate userid-eid matrix and Decompose it\n",
    "    Truncated SVD\n",
    "'''\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_user_event_matrix(dict_, u_sample, user2ind, binary=False):\n",
    "    '''Get (user,event) matrix.\n",
    "    This matrix will be decomposed by TruncatedSVD (or else?)\n",
    "    Only users in u_sample are considered.\n",
    "    '''\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    jj = 0\n",
    "    eid2ind = {}\n",
    "    for ii, (eid, value) in tqdm(enumerate(dict_.items())):\n",
    "        user_in_event = get_user_in_event(dict_, eid, u_sample)\n",
    "        if len(user_in_event)==0:\n",
    "            # No user in u_sample appears in this eid event.\n",
    "            continue\n",
    "        else:\n",
    "            eid2ind[eid] = jj\n",
    "#             eind = eid2ind[eid]\n",
    "        for uid, nb_occur in user_in_event:\n",
    "            uind = user2ind[uid]\n",
    "            col.append(jj)\n",
    "            row.append(uind)\n",
    "            if binary:\n",
    "                data.append(1)    # Binary matrix\n",
    "            else:\n",
    "                data.append(nb_occur)\n",
    "        jj+=1\n",
    "    print(\"{} events have at least one user in u_sample\".format(jj))\n",
    "    print(\"{} events have no user in u_sample\".format(len(dict_)-jj))\n",
    "    return csr_matrix((data, (row, col)), shape=(len(user2ind), len(eid2ind))), eid2ind\n",
    "\n",
    "\n",
    "#train\n",
    "matrix_sub_train, eid_sample2ind_train = get_user_event_matrix(train_dict_, u_sample_train, user_sample2ind_train, binary=True)\n",
    "matrix_main_train, eid_main2ind_train = get_user_event_matrix(train_dict_, u_pop_train, user2ind_train, binary=True)\n",
    "matrix_main_cnt_train, eid_main_cnt2ind_train = get_user_event_matrix(train_dict_, u_pop_train, user2ind_train, binary=False)\n",
    "print('train:')\n",
    "print(\"matrix_sub shape : {}\".format(matrix_sub_train.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_sub_train.count_nonzero()/(matrix_sub_train.shape[0]*matrix_sub_train.shape[1])))\n",
    "print(\"matrix_main shape : {}\".format(matrix_main_train.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_main_train.count_nonzero()/(matrix_main_train.shape[0]*matrix_main_train.shape[1])))\n",
    "\n",
    "\n",
    "#test\n",
    "matrix_sub_test, eid_sample2ind_test = get_user_event_matrix(test_dict_, u_sample_test, user_sample2ind_test, binary=True)\n",
    "matrix_main_test, eid_main2ind_test = get_user_event_matrix(test_dict_, u_pop_test, user2ind_test, binary=True)\n",
    "matrix_main_cnt_test, eid_main_cnt2ind_test = get_user_event_matrix(test_dict_, u_pop_test, user2ind_test, binary=False)\n",
    "print('test:')\n",
    "print(\"matrix_sub shape : {}\".format(matrix_sub_test.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_sub_test.count_nonzero()/(matrix_sub_test.shape[0]*matrix_sub_test.shape[1])))\n",
    "print(\"matrix_main shape : {}\".format(matrix_main_test.shape))\n",
    "print(\"Sparsity : {}\".format(matrix_main_test.count_nonzero()/(matrix_main_test.shape[0]*matrix_main_test.shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test user_feature shape : (4335, 100)\n",
      "train user_feature_sub shape : (20000, 100)\n",
      "test user_feature_sub shape : (4335, 100)\n",
      "SVD is done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "RELOAD = False\n",
    "\n",
    "if RELOAD:\n",
    "    ### Load matrix_main\n",
    "    nb_feature_main = 10     # 10 for weibo, 20 for tweet\n",
    "    \n",
    "    #train\n",
    "    u_main_train = np.load(open('matrix/weibo_u_main_train.npy','rb'))\n",
    "    sigma_main_train = np.load(open('matrix/weibo_sigma_main_train.npy','rb'))\n",
    "    vt_main_train = np.load(open('matrix/weibo_vt_main_train.npy','rb'))\n",
    "    user_feature_train = u_main_train.dot(np.diag(sigma_main_train))\n",
    "    print(\"train user_feature shape : {}\".format(user_feature_train.shape))\n",
    "    \n",
    "    \n",
    "    #test\n",
    "    u_main_test = np.load(open('matrix/weibo_u_main_test.npy','rb'))\n",
    "    sigma_main_test = np.load(open('matrix/weibo_sigma_main_test.npy','rb'))\n",
    "    vt_main_test = np.load(open('matrix/weibo_vt_main_test.npy','rb'))\n",
    "    \n",
    "    user_feature_test = u_main_test.dot(np.diag(sigma_main_test))\n",
    "    print(\"test user_feature shape : {}\".format(user_feature_test.shape))\n",
    "\n",
    "    ### Load matrix_sub\n",
    "    \n",
    "    #train\n",
    "    u_sub_train = np.load(open('matrix/weibo_u_sub_train.npy','rb'))\n",
    "    sigma_sub_train = np.load(open('matrix/weibo_sigma_sub_train.npy','rb'))\n",
    "    vt_sib_train = np.load(open('matrix/weibo_vt_sub_train.npy','rb'))\n",
    "    user_feature_sub_train = u_sub_train.dot(np.diag(sigma_sub_train))\n",
    "    \n",
    "    #test\n",
    "    u_sub_test = np.load(open('matrix/weibo_u_sub_test.npy','rb'))\n",
    "    sigma_sub_test = np.load(open('matrix/weibo_sigma_sub_test.npy','rb'))\n",
    "    vt_sib_test = np.load(open('matrix/weibo_vt_sub_test.npy','rb'))\n",
    "    user_feature_sub_test = u_sub_test.dot(np.diag(sigma_sub_test))\n",
    "    \n",
    "    nb_feature_sub = 50\n",
    "    print(\"train user_feature_sub shape : {}\".format(user_feature_sub_train.shape))\n",
    "    print(\"test user_feature_sub shape : {}\".format(user_feature_sub_test.shape))\n",
    "    print(\"Loading is Done.\")\n",
    "else:\n",
    "    nb_feature_main = 10     # 10 for weibo, 20 for tweet\n",
    "    n_iter = 15    # 15 for weibo, 7 for tweet\n",
    "    \n",
    "    #train\n",
    "    u_main_train, sigma_main_train, vt_main_train = randomized_svd(matrix_main_train, n_components=100,\n",
    "                                                 n_iter=n_iter, random_state=42)\n",
    "    user_feature_train = u_main_train.dot(np.diag(sigma_main_train))\n",
    "    print(\"train user_feature shape : {}\".format(user_feature_train.shape))\n",
    "    \n",
    "    \n",
    "    #test\n",
    "    u_main_test, sigma_main_test, vt_main_test = randomized_svd(matrix_main_test, n_components=100,\n",
    "                                                 n_iter=n_iter, random_state=42)\n",
    "    user_feature_test = u_main_test.dot(np.diag(sigma_main_test))\n",
    "    print(\"test user_feature shape : {}\".format(user_feature_test.shape))\n",
    "\n",
    "    nb_feature_sub = 50\n",
    "    \n",
    "    #train\n",
    "    matrix_sub_train = matrix_sub_train.dot(matrix_sub_train.transpose())\n",
    "    matrix_sub_array_train = matrix_sub_train.toarray()\n",
    "    u_sub_train, sigma_sub_train, vt_sub_train = randomized_svd(matrix_sub_train, n_components=100,\n",
    "                                              n_iter=n_iter, random_state=42)  # random_state=42\n",
    "    user_feature_sub_train = u_sub_train.dot(np.diag(sigma_sub_train))\n",
    "    \n",
    "    #test\n",
    "    matrix_sub_test = matrix_sub_test.dot(matrix_sub_test.transpose())\n",
    "    matrix_sub_array_test = matrix_sub_test.toarray()\n",
    "    u_sub_test, sigma_sub_test, vt_sub_test = randomized_svd(matrix_sub_test, n_components=100,\n",
    "                                              n_iter=n_iter, random_state=42)  # random_state=42\n",
    "    user_feature_sub_test = u_sub_test.dot(np.diag(sigma_sub_test))\n",
    "    \n",
    "    \n",
    "    print(\"train user_feature_sub shape : {}\".format(user_feature_sub_train.shape))\n",
    "    print(\"test user_feature_sub shape : {}\".format(user_feature_sub_test.shape))\n",
    "    print(\"SVD is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving matrices\n",
    "if not RELOAD:\n",
    "    #train\n",
    "    np.save('matrix/weibo_u_main_train.npy',u_main_train)\n",
    "    np.save('matrix/weibo_sigma_main_train.npy',sigma_main_train)\n",
    "    np.save('matrix/weibo_vt_main_train.npy',vt_main_train)\n",
    "    \n",
    "    np.save('matrix/weibo_u_sub_train.npy',u_sub_train)\n",
    "    np.save('matrix/weibo_sigma_sub_train.npy',sigma_sub_train)\n",
    "    np.save('matrix/weibo_vt_sub_train.npy',vt_sub_train)\n",
    "    \n",
    "    #test\n",
    "    np.save('matrix/weibo_u_main_test.npy',u_main_test)\n",
    "    np.save('matrix/weibo_sigma_main_test.npy',sigma_main_test)\n",
    "    np.save('matrix/weibo_vt_main_test.npy',vt_main_test)\n",
    "    \n",
    "    np.save('matrix/weibo_u_sub_test.npy',u_sub_test)\n",
    "    np.save('matrix/weibo_sigma_sub_test.npy',sigma_sub_test)\n",
    "    np.save('matrix/weibo_vt_sub_test.npy',vt_sub_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daac1ae858242dd8f9997e10ae8c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length of sentences : 24401\n",
      "test length of sentences : 3281\n"
     ]
    }
   ],
   "source": [
    "import jieba, re\n",
    "\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "threshold = 90*24\n",
    "resolution = 'hour'\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "chinese_stopwords = '、 。 〃 〄 々 〆 〇 〈〉 《 》 「 」 『 』 【】 〒 〓 〔 〕 〖 〗 〘〙 〚 〛 〛 〜 〝 〞 〟，'\n",
    "rx = '[' + re.escape(''.join(chinese_stopwords.split())) + ']'\n",
    "\n",
    "\n",
    "def get_sentences(dict_,eid_list):\n",
    "    sentences = []\n",
    "    for eid in tqdm(eid_list):\n",
    "        eid = str(eid)\n",
    "        messages = dict_[eid]\n",
    "        ts = np.array(messages['timestamps'], dtype=np.int32)\n",
    "        text_seq = np.array(messages['text'])\n",
    "\n",
    "        if resolution=='day':\n",
    "            binsize = 3600*24\n",
    "        elif resolution=='hour':\n",
    "            binsize = 3600\n",
    "        elif resolution=='minute':\n",
    "            binsize = 60\n",
    "        ts2 = sorted(ts)\n",
    "        cnt, bins = np.histogram(ts2, bins=range(ts2[0],ts2[0]+threshold*binsize,binsize))\n",
    "\n",
    "\n",
    "        nonzero_bins_ind = np.nonzero(cnt)[0]\n",
    "        nonzero_bins = bins[nonzero_bins_ind]\n",
    "        hist = cnt[nonzero_bins_ind]\n",
    "        inv = nonzero_bins_ind[1:]-nonzero_bins_ind[:-1]\n",
    "        intervals = np.insert(inv,0,0)\n",
    "\n",
    "        for bid, bin_left in enumerate(nonzero_bins):\n",
    "            bin_right = bin_left + binsize\n",
    "            try:\n",
    "                del doc\n",
    "            except:\n",
    "                pass\n",
    "            # Collecting text to make doc\n",
    "            for tid, t in enumerate(ts):\n",
    "                if t<bin_left:\n",
    "                    continue\n",
    "                elif t>=bin_right:\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "                string = text_seq[tid]\n",
    "                string = re.sub(r\"http\\S+\", \"\", string)\n",
    "                string = re.sub(\"[?!.,:;()'@#$%^&*-=+/\\[\\[\\]\\]]\", ' ', string) # !.,:;()'@#$%^&*-_{}=+/\\\"\n",
    "                try:\n",
    "                    doc += string\n",
    "                except:\n",
    "                    doc = string\n",
    "            if isinstance(eid, int):\n",
    "                eid_str = str(eid)\n",
    "            else:\n",
    "                eid_str = eid\n",
    "            sentences.append(TaggedDocument(words=jieba.lcut(doc), tags=[eid_str+'_%s' % bid]))\n",
    "                \n",
    "    return sentences\n",
    "    \n",
    "train_sentences = get_sentences(train_dict_,eid_train)\n",
    "\n",
    "print(\"train length of sentences : {}\".format(len(train_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omranpoor/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocab is done.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "reload = False\n",
    "if reload:\n",
    "    doc_vectorizer = Doc2Vec.load('weibo_docTovec.model')\n",
    "    print(\"doc_vectorizer is loaded.\")\n",
    "else:\n",
    "    doc_vectorizer = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "    doc_vectorizer.build_vocab(train_sentences)\n",
    "    print(\"build_vocab is done.\")\n",
    "    doc_vectorizer.train(train_sentences,total_examples=doc_vectorizer.corpus_count,epochs=10)\n",
    "    print(\"doc2vec training is done.\")\n",
    "    doc_vectorizer.save('weibo_docTovec_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115577, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_user_feature_in_event(dict_, eid, u_sample, user_feature_sub, user_sample2ind):\n",
    "    '''Get user_feature_sub matrix for event eid'''\n",
    "    user_in_event = get_user_in_event(dict_, eid, u_sample)\n",
    "    nb_feature = user_feature_sub.shape[1]\n",
    "    \n",
    "    for uid, nb_occur in user_in_event:\n",
    "        uind = user_sample2ind[uid]\n",
    "        feature_vec = user_feature_sub[uind,:].reshape(1,-1)\n",
    "        try:\n",
    "            ret_matrix = np.concatenate((ret_matrix, feature_vec), axis=0)\n",
    "        except:\n",
    "            ret_matrix = feature_vec\n",
    "    try:\n",
    "        return ret_matrix\n",
    "    except:\n",
    "        ### if user_in_event is empty\n",
    "        return np.zeros((1,nb_feature))\n",
    "\n",
    "def create_dataset(dict_, eid, threshold=90, resolution='day',\n",
    "                   read_text=False, embeddings_index=None, stopwords=None,\n",
    "                   doc2vec_model=None, user_feature=None, user2ind=None, read_user=False, task='regression',\n",
    "                   cutoff=50, return_useridx=True):\n",
    "    messages = dict_[eid]\n",
    "    ts = np.array(messages['timestamps'], dtype=np.int32)\n",
    "    try:\n",
    "        user_list = messages['uid'].tolist()\n",
    "    except:\n",
    "        user_list = messages['uid']\n",
    "    if read_text:\n",
    "        text_seq = np.array(messages['text'])\n",
    "    else:\n",
    "        text_seq = None\n",
    "    if read_user:\n",
    "        XX, XX_uidx = get_features(dict_,eid, ts, threshold=threshold, resolution=resolution, read_text=read_text,\n",
    "                              text_seq=text_seq, embeddings_index=embeddings_index, stopwords=stopwords,\n",
    "                              doc2vec_model=doc2vec_model, read_user=read_user,\n",
    "                              user_feature=user_feature, user2ind=user2ind, user_list=user_list,\n",
    "                              cutoff=cutoff, return_useridx=return_useridx)\n",
    "    else:\n",
    "        XX = get_features(dict_,eid, ts, threshold=threshold, resolution=resolution, read_text=read_text,\n",
    "                              text_seq=text_seq, embeddings_index=embeddings_index, stopwords=stopwords,\n",
    "                              doc2vec_model=doc2vec_model, read_user=read_user,\n",
    "                              user_feature=user_feature, user2ind=user2ind, user_list=user_list,\n",
    "                              cutoff=cutoff, return_useridx=return_useridx)\n",
    "\n",
    "    if task==\"regression\":\n",
    "        X = XX[:-1,:]   # (nb_sample, 2+)\n",
    "        y = XX[1:,:2]\n",
    "#         y = XX[1:,1]\n",
    "        if len(y.shape)==1:\n",
    "            return X, y.reshape(-1,1)\n",
    "        elif len(y.shape)==2:\n",
    "            return X, y\n",
    "    elif task==\"classification\":\n",
    "        X = XX   # (nb_sample, 2+)\n",
    "        y = int(messages['label'])\n",
    "        if return_useridx:\n",
    "            return X, XX_uidx, y\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "\n",
    "def get_features(dict_,eid, timestamps, threshold=90, resolution='day', sep=False, read_text=False,\n",
    "                 text_seq=None, embeddings_index=None, stopwords=None, read_user=False,\n",
    "                 doc2vec_model=None, user_feature=None, user2ind=None, user_list=None,\n",
    "                 cutoff=50, return_useridx=True):\n",
    "    '''\n",
    "    timestamps\n",
    "        : relative timestamps since the first tweet\n",
    "        : it should be sorted.\n",
    "        : unit = second\n",
    "    unit of threshold and resolution should be matched.\n",
    "    '''\n",
    "    ts = timestamps\n",
    "    if resolution=='day':\n",
    "        binsize = 3600*24\n",
    "    elif resolution=='hour':\n",
    "        binsize = 3600\n",
    "    elif resolution=='minute':\n",
    "        binsize = 60\n",
    "    ts2 = sorted(ts)\n",
    "    cnt, bins = np.histogram(ts2, bins=range(ts2[0],ts2[0]+threshold*binsize,binsize))\n",
    "                             \n",
    "    nonzero_bins_ind = np.nonzero(cnt)[0]\n",
    "    nonzero_bins = bins[nonzero_bins_ind]\n",
    "    \n",
    "    hist = cnt[nonzero_bins_ind]\n",
    "    inv = nonzero_bins_ind[1:]-nonzero_bins_ind[:-1]\n",
    "    intervals = np.insert(inv,0,0)\n",
    "    ### Cutoff sequence\n",
    "#     cutoff = 50\n",
    "    if len(hist)>cutoff:\n",
    "        hist = hist[:cutoff]\n",
    "        intervals = intervals[:cutoff]\n",
    "        nonzero_bins = nonzero_bins[:cutoff]\n",
    "\n",
    "    ### user feature   \n",
    "    if read_user:\n",
    "        X_useridx = []\n",
    "        for bid, bin_left in enumerate(nonzero_bins):\n",
    "            bin_userlist = []\n",
    "            bin_right = bin_left + binsize\n",
    "            try:\n",
    "                del temp\n",
    "            except:\n",
    "                pass\n",
    "            # Collecting text to make doc\n",
    "            for tid, t in enumerate(ts):\n",
    "                if t<bin_left:\n",
    "                    continue\n",
    "                elif t>=bin_right:\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "                uid = user2ind[user_list[tid]]\n",
    "                bin_userlist.append(user_list[tid])\n",
    "                coef = user_feature[uid,:].reshape(1,-1)   # (1,n_components)\n",
    "                try:\n",
    "                    temp = np.concatenate((temp, coef), axis=0)\n",
    "                except:\n",
    "                    temp = coef\n",
    "\n",
    "            X_user_bin = np.mean(temp, axis=0).reshape(1,-1)\n",
    "\n",
    "            try:\n",
    "                X_user = np.concatenate((X_user, X_user_bin), axis=0)\n",
    "            except:\n",
    "                X_user = X_user_bin\n",
    "            X_useridx.append(bin_userlist)\n",
    "\n",
    "    ### text feature\n",
    "    if read_text:\n",
    "        text_matrix = get_doc2vec(dict_,doc2vec_model, eid, nonzero_bins)\n",
    "    \n",
    "    if sep:\n",
    "        if read_text:\n",
    "            return hist, intervals, X_user, text_matrix\n",
    "        else:\n",
    "            return hist, intervals, X_user\n",
    "    else:\n",
    "        if read_text and read_user:\n",
    "            if return_useridx:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user, text_matrix]), X_useridx\n",
    "            else:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user, text_matrix])\n",
    "        elif read_text or read_user:\n",
    "            if read_text:\n",
    "                return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), text_matrix])\n",
    "            elif read_user:\n",
    "                if return_useridx:\n",
    "                    return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user]), X_useridx\n",
    "                else:\n",
    "                    return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1), X_user])\n",
    "        else:\n",
    "            return np.hstack([hist.reshape(-1,1), intervals.reshape(-1,1)])\n",
    "    \n",
    "def get_doc2vec(dict_,doc2vec_model, eid, nonzero_bins):\n",
    "    X_text = np.array([])\n",
    "    if isinstance(eid, int):\n",
    "            eid_str = str(eid)\n",
    "    else:\n",
    "        eid_str = eid\n",
    "    ts = np.array(dict_[eid_str]['timestamps'], dtype=np.int32)\n",
    "    text_seq= np.array(dict_[eid_str]['text'])\n",
    "    for bid, bin_left in enumerate(nonzero_bins):\n",
    "#         tag = eid_str+'_'+str(bid)\n",
    "#         temp = doc2vec_model.docvecs[tag] # (300,)\n",
    "        bin_right = bin_left + binsize\n",
    "        doc = ''\n",
    "        for tid, t in enumerate(ts):\n",
    "            if t<bin_left:\n",
    "                continue\n",
    "            elif t>=bin_right:\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            string = text_seq[tid]\n",
    "            string = re.sub(r\"http\\S+\", \"\", string)\n",
    "            string = re.sub(\"[?!.,:;()'@#$%^&*-=+/\\[\\[\\]\\]]\", ' ', string) # !.,:;()'@#$%^&*-_{}=+/\\\"\n",
    "            doc += string\n",
    "        temp = doc2vec_model.infer_vector(jieba.lcut(doc))\n",
    "        temp = temp.reshape(1,-1)\n",
    "        if X_text.shape[0] != 0:\n",
    "            X_text = np.concatenate((X_text, temp), axis=0)\n",
    "        else:\n",
    "            X_text = temp\n",
    "    return X_text\n",
    "    \n",
    "### Building Model\n",
    "LOAD_MODEL = False\n",
    "task = \"classification\"  #\"classification\"\n",
    "\n",
    "#train\n",
    "scaler_dict_train = {}\n",
    "noerr_eid_list_train = set()\n",
    "X_dict_train = {}\n",
    "X_uidx_dict_train = {}\n",
    "subX_dict_train = {}\n",
    "y_dict_train = {}\n",
    "\n",
    "#test\n",
    "scaler_dict_test = {}\n",
    "noerr_eid_list_test = set()\n",
    "X_dict_test = {}\n",
    "X_uidx_dict_test = {}\n",
    "subX_dict_test = {}\n",
    "y_dict_test = {}\n",
    "\n",
    "\n",
    "nb_rumor = 0\n",
    "burnin = 5 if task==\"regression\" else 0\n",
    "read_text = True\n",
    "read_user = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a71762d07d4cf7aab2871208fad757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=455), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "for eid in tqdm(eid_train):\n",
    "    eid = str(eid)\n",
    "    if read_user:\n",
    "        X, X_uidx, y = create_dataset(train_dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature_train[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind_train, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=True)\n",
    "    else:\n",
    "        X, y = create_dataset(train_dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature_train[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind_train, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=False)\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "        continue\n",
    "\n",
    "    X_dict_train[eid] = X\n",
    "    if read_user:\n",
    "        X_uidx_dict_train[eid] = X_uidx\n",
    "    subX_dict_train[eid] = get_user_feature_in_event(train_dict_, eid, u_sample_train, \n",
    "                                               user_feature_sub_train[:,:nb_feature_sub], user_sample2ind_train)\n",
    "    y_dict_train[eid] = y\n",
    "\n",
    "    try:\n",
    "        scaler_dict_train[eid]\n",
    "    except:\n",
    "        scaler_hist = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_hist.fit(X[:,0].reshape(-1,1))\n",
    "        scaler_interval = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_interval.fit(X[:,1].reshape(-1,1))\n",
    "        scaler_dict_train[eid] = (scaler_hist, scaler_interval)\n",
    "        \n",
    "        \n",
    "#test\n",
    "for eid in tqdm(eid_test):\n",
    "    eid = str(eid)\n",
    "    if read_user:\n",
    "        X, X_uidx, y = create_dataset(test_dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature_test[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind_test, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=True)\n",
    "    else:\n",
    "        X, y = create_dataset(test_dict_, eid, threshold=90*24, resolution='hour',\n",
    "                                 read_text=read_text, embeddings_index=None, stopwords=None,\n",
    "                                 doc2vec_model=doc_vectorizer, user_feature=user_feature_test[:,:nb_feature_main], \n",
    "                                 user2ind=user2ind_test, read_user=read_user, task=task, cutoff=50,\n",
    "                                 return_useridx=False)\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "        continue\n",
    "\n",
    "    X_dict_test[eid] = X\n",
    "    if read_user:\n",
    "        X_uidx_dict_test[eid] = X_uidx\n",
    "    subX_dict_test[eid] = get_user_feature_in_event(test_dict_, eid, u_sample_test, \n",
    "                                               user_feature_sub_test[:,:nb_feature_sub], user_sample2ind_test)\n",
    "    y_dict_test[eid] = y\n",
    "\n",
    "    try:\n",
    "        scaler_dict_test[eid]\n",
    "    except:\n",
    "        scaler_hist = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_hist.fit(X[:,0].reshape(-1,1))\n",
    "        scaler_interval = MinMaxScaler(feature_range=(0,1))\n",
    "        scaler_interval.fit(X[:,1].reshape(-1,1))\n",
    "        scaler_dict_test[eid] = (scaler_hist, scaler_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI model\n",
    "- Python : 2.7.x\n",
    "- Keras : 1.2.1\n",
    "- Theano : 0.9.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is compiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:45: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_regularizer=<keras.reg..., activation=\"tanh\")`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ad...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:64: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ad...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:65: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "matrix_main is used for LSTM input.\n",
    "matrix_sub is used for the scoring module.\n",
    "'''\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Input, Dropout, Lambda, LSTM, Embedding, Conv1D, TimeDistributed, Add\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "               \n",
    "#configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "acc=0\n",
    "\n",
    "nb_features = 2+10+100    # (#temporal, #user, #doc)\n",
    "dim_hidden = 50\n",
    "\n",
    "##### Main part #####\n",
    "inputs = Input(shape=(None, nb_features))\n",
    "emb_out = TimeDistributed(Dense(100, activation='tanh'))(inputs)    # W_e\n",
    "emb_out = Dropout(0.2)(emb_out)\n",
    "rnn_out = LSTM(dim_hidden, activation='tanh', return_sequences=False)(emb_out)    #(None, dim_hidden)\n",
    "rnn_out = Dense(100, activation='tanh')(rnn_out)     # (None, 100) W_r\n",
    "rnn_out = Dropout(0.2)(rnn_out)\n",
    "\n",
    "\n",
    "##### Sub part #####\n",
    "nb_score = 1\n",
    "nb_expand = 100\n",
    "sub_input = Input(shape=(None, nb_feature_sub))\n",
    "user_vec = TimeDistributed(Dense(nb_expand, activation='tanh',\n",
    "                                 W_regularizer=regularizers.l2(0.01)))(sub_input)   # (None, None, nb_expand)\n",
    "sub_h = TimeDistributed(Dense(nb_score, activation='sigmoid'))(user_vec)    # (None, None, nb_score)\n",
    "z = Lambda(lambda x: K.mean(x, axis=1), output_shape=lambda s: (s[0], s[2]))(sub_h)    #(None, nb_score)\n",
    "\n",
    "##### Concatenate #####\n",
    "out1 = Dense(1, activation='sigmoid')(rnn_out)\n",
    "concat_out = Add()([out1, z])\n",
    "# concat_out = merge([rnn_out, z], mode='concat', concat_axis=1)\n",
    "# concat_out = concatenate([rnn_out, z], axis=1)\n",
    "\n",
    "##### Classifier #####\n",
    "# outputs = Dense(1, activation='sigmoid')(concat_out)\n",
    "# outputs = Dense(1, activation='sigmoid')(concat_out)\n",
    "outputs = concat_out\n",
    "\n",
    "\n",
    "##### Model #####\n",
    "hvector = Model(input=[inputs, sub_input], output=concat_out)\n",
    "zscore = Model(input=sub_input, output=sub_h)\n",
    "model = Model(input=[inputs, sub_input], output=outputs)\n",
    "uvector = Model(input=sub_input, output=user_vec)\n",
    "# model = Model(input=inputs, output=outputs)\n",
    "\n",
    "\n",
    "##### Compile #####\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "if task==\"regression\":\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='mean_squared_error')\n",
    "elif task==\"classification\":\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='binary_crossentropy')\n",
    "print(\"Model is compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3777, 455)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_dict_train.keys()) , len(X_dict_test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c244a43a2dd945498afaf54f043d4b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3c5ddbea084b73ad1b2d480594ccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.9409028450576976  train acc: 0.7866031241726238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751a60385ec84e45a467f0ab59d1e4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.45206826486271434  train acc: 0.9144823934339423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9181420ba8f440748c561c1abe7ea1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.36045671918034666  train acc: 0.9144823934339423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac10a0e10cdd4b928188499f1ea05606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.3033729295231755  train acc: 0.9189833200953137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d0e0238d6b4e6bb48f3cffa2c3c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.30972319877730387  train acc: 0.924013767540376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a55f88a3f9f4123ac3e769dff0aaa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2755113806198767  train acc: 0.9229547259729944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4200d9e1b8454595a87646b831b57588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2597500276099619  train acc: 0.9229547259729944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b815e2ec6d274d638439a08520da4aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2599901805651789  train acc: 0.9197776012708498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9e2f40f3fd4d9b92af341b41307ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2334732790412231  train acc: 0.9356632247815727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df126ba6384479eb9867b364d8768b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24174157251387654  train acc: 0.9343394228223457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2008dc5b7ba4be2a80021c42a95ac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.25804582966759365  train acc: 0.9131585914747153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b513a549c748f6aeda4f717f00e120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.22703588017782056  train acc: 0.938046068308181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27cf42a3a8b4fdc89540e549db8e60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.24451553247802932  train acc: 0.931691818903892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341958a5d494441ca4afccb7f6c20575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.23248746707273052  train acc: 0.9377813079163357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a20076b2ad4d688e665142ef87de62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.22818226208784592  train acc: 0.9417527137940164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52af919994044bc4a21d928df5370530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.22003234124221924  train acc: 0.9428117553613979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00213d487144484ebf2bacbc60c3f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2034245376670691  train acc: 0.9459888800635425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1c835c6dbd40508fe9be37ec2a9c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20616405606289562  train acc: 0.9457241196716971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc409a28b265480cadb978bf9256569b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1971735328841271  train acc: 0.9457241196716971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac1dae5f784a3ca885cb972d345a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.21812261817846146  train acc: 0.9473126820227694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513294905db944828e1db0fd45ed9dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1983312834170123  train acc: 0.9465184008472333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bb2f9432e046078d7b8964c689eca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19125956989753756  train acc: 0.949166004765687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d0dbc4d5794f4e9564bcd4cb9848c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19891435473869823  train acc: 0.9515488482922955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a482bd313f1b48c490c61ae5d431e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1956644687685215  train acc: 0.9515488482922955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e38fc0a74d04da599f60a0a53e5dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19184887898530587  train acc: 0.9510193275086047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1632db603e4548a09751a922a0b4f304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1969227563669456  train acc: 0.9494307651575324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a9fad0094e47c387cb51e6e8e9ed32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.19051790575681538  train acc: 0.9552554937781308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214839fc72b64b548651a3a2c9c508c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.189438936998143  train acc: 0.9531374106433678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5a0bebe8f94d5cb3841541670156c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1993879346510601  train acc: 0.9547259729944401\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7e5d99f394ca39f962cbdf1986410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20067511170733643  train acc: 0.9428117553613979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd65ec36f7a4493986d903852edc9f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20430181301052186  train acc: 0.9422822345777072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7311767244548c39c052f3098823bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18142007219020512  train acc: 0.95128408790045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181a43a13a094a27be0a705625b7c6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17980969962332302  train acc: 0.9541964522107492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7e27de1b7e4277a2ca02098711ff9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1946598623787034  train acc: 0.95816785808843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f994632fc63484c9ac66fd88983c713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17812044422250947  train acc: 0.9597564204395023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31793abceb5f4ad592d10aa883850569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17539752887077145  train acc: 0.9534021710352132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769fdb019de4d9e8d7ce546b0ced201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17958060772939116  train acc: 0.9592268996558115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1aa9936d2bb414cbf49c1e310592622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17800284877060948  train acc: 0.9605507016150384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed912ad939c439c9c53d0eb5f126f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3777), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.17745041604880477  train acc: 0.9589621392639661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from keras.models import load_model\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "### Training... ###\n",
    "# acc = 0\n",
    "nb_epoch = 40\n",
    "y_val = [int(dict_[str(eid)]['label']) for eid in eid_val]\n",
    "y_val = np.array(y_val) > 0.5\n",
    "train_acc = 0\n",
    "val_acc = 0\n",
    "    \n",
    "for ep in tqdm(range(nb_epoch)):\n",
    "    ##### Looping for eid_train #####\n",
    "    train_losses = []\n",
    "    train_preds = []\n",
    "    y_train = []\n",
    "    for eid in tqdm(eid_train):\n",
    "        if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "            continue\n",
    "        eid = str(eid)\n",
    "        X = X_dict_train[eid]\n",
    "        X = X.astype(np.float32)\n",
    "        y = y_dict_train[eid]\n",
    "\n",
    "        label = int(train_dict_[eid]['label'])\n",
    "        noerr_eid_list_train.add(eid)\n",
    "        sh = scaler_dict_train[eid][0]\n",
    "        si = scaler_dict_train[eid][1]\n",
    "        \n",
    "        ##### Main input #####\n",
    "        trainX = X\n",
    "        ##### Sub input #####\n",
    "        sub_trainX = subX_dict_train[eid]\n",
    "        \n",
    "        trainY = y\n",
    "        dim_output = 1\n",
    "        y_train.append(y)\n",
    "\n",
    "        h = model.fit([trainX[np.newaxis,:,:], sub_trainX[np.newaxis,:,:]], np.array([trainY]), \n",
    "                      batch_size=1, epochs=1, verbose=0)\n",
    "        pred = model.predict([trainX[np.newaxis,:,:], sub_trainX[np.newaxis,:,:]],batch_size=1,verbose=0)\n",
    "        train_preds.append(pred[0,0])\n",
    "        train_losses.append(h.history['loss'][0])\n",
    "\n",
    "    train_preds = np.array(train_preds)\n",
    "    train_preds = train_preds>0.5\n",
    "    y_train = np.array(y_train) > 0.5\n",
    "    \n",
    "    ### Evaluation On Validation###\n",
    "    val_preds = []\n",
    "    for eid in eid_val:\n",
    "        if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "            continue\n",
    "        eid = str(eid)\n",
    "        X = X_dict[eid]\n",
    "        X = X.astype(np.float32)\n",
    "        y = y_dict[eid]\n",
    "        valX = X\n",
    "        sub_valX = subX_dict[eid]\n",
    "        \n",
    "        pred = model.predict([np.array([valX]), np.array([sub_valX])], verbose=0)\n",
    "        val_preds.append(pred[0,0])\n",
    "    \n",
    "    val_preds = np.array(val_preds)\n",
    "    val_preds = val_preds > 0.5\n",
    "    \n",
    "    \n",
    "    #results\n",
    "    train_acc = np.mean(train_preds == y_train)\n",
    "    val_acc = np.mean(val_preds == y_val)\n",
    "    print(\"train loss:\",np.mean(train_losses),' train acc:',train_acc,' val acc':val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f788e98083d46dfaf75c54e5f301d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=455), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision_fake': 0.7321428571428571, 'fscore_real': 0.7714987714987716, 'precision_real': 0.8971428571428571, 'val': 0.9120370370370371, 'recall_fake': 0.9192825112107623, 'fscore_fake': 0.8151093439363817, 'recall_real': 0.6767241379310345, 'te': 0.7956043956043956, 'tr': 0.9589621392639661}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test\n",
    "test_preds = []\n",
    "y_test = [int(test_dict_[str(eid)]['label']) for eid in eid_test]\n",
    "y_test = np.array(y_test) > 0.5\n",
    "\n",
    "for eid in tqdm(eid_test):\n",
    "        if X.shape[0]<=2*burnin:  # ignore length<=1 sequence\n",
    "            continue\n",
    "        eid = str(eid)\n",
    "        X = X_dict_test[eid]\n",
    "        X = X.astype(np.float32)\n",
    "        y = y_dict_test[eid]\n",
    "\n",
    "        testX = X\n",
    "        sub_testX = subX_dict_test[eid]\n",
    "        pred = model.predict([np.array([testX]), np.array([sub_testX])], verbose=0)\n",
    "        test_preds.append(pred[0,0])\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "test_preds = test_preds>0.5\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_preds).ravel()\n",
    "precision_real = tp/(tp+fp)\n",
    "precision_fake = tn/(fn+tn)\n",
    "recall_real = tp/(tp+fn)\n",
    "recall_fake = tn/(fp+tn)\n",
    "test_acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "fscore_real = 2*tp/(2*tp+fp+fn)\n",
    "fscore_fake = 2*tn/(2*tn+fp+fn)\n",
    "\n",
    "ft = open('csi_accuracy_weibo.txt','w')\n",
    "result = {'train_acc':train_acc,'test_acc':test_acc,'val_acc':val_acc,'precision_real':precision_real,'precision_fake':precision_fake,'recall_real':recall_real,'recall_fake':recall_fake,'fscore_real':fscore_real,'fscore_fake':fscore_fake}\n",
    "print(str(result))\n",
    "json.dump(str(result),ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
